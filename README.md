# ğŸ§  Fake News Detection using Transformer Models  
### *A Comparative Study of Human vs AI Judgment*

---

## ğŸ“„ Project Overview
This project investigates how **Artificial Intelligence** compares to **human intuition** in detecting fake news.  
A transformer-based NLP model (**DistilBERT**) was fine-tuned on English news articles to classify them as *real* or *fake*, and its performance was evaluated against a human participantâ€™s manual judgments.

---

## ğŸ¯ Objectives
- Build a reproducible end-to-end fake news detection pipeline.  
- Clean and normalize the Kaggle *Fake/Real News* dataset.  
- Establish a **TF-IDF + Logistic Regression** baseline.  
- Fine-tune **DistilBERT** and compare its accuracy to human evaluation.  
- Analyze misclassification patterns and discuss implications for AI trust and interpretability.

---

## ğŸ“š Verified Literature (2023â€“2025)

1. **Ramzan, A., Ali, R.â€¯H., Ali, N., & Khan, A. (2024).** *Enhancing Fake News Detection Using BERT: A Comparative Analysis of Logistic Regression, RFC, LSTM and BERT.* In 2024 International Conference on IT and Industrial Technologies (ICIT). IEEE. DOI: [10.1109/ICIT63607.2024.10859673](https://doi.org/10.1109/ICIT63607.2024.10859673)

2. **Kitanovski, M., & Mitrevski, P. (2023).** *DistilBERT and RoBERTa Models for Identification of Fake News.* 46th MIPRO ICT and Electronics Convention. IEEE. DOI: [10.23919/MIPRO57284.2023.10159740](https://doi.org/10.23919/MIPRO57284.2023.10159740)

3. **Saadi, A., Belhadef, H., Guessas, A., & Hafirassou, O. (2025).** *Enhancing Fake News Detection with Transformer Models and Summarization.* *Engineering, Technology & Applied Science Research, 15*(3), 23253â€“23259. DOI: [10.48084/etasr.10678](https://doi.org/10.48084/etasr.10678)

*Dataset citation:* Kaggle. *Fake and Real News Dataset.* Retrieved 2025. [Link](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset)

---

## ğŸ“Š Dataset Collection & Preparation

**Source:** Kaggle â€“ *Fake and Real News Dataset*  
**Files used:** `Fake.csv` (23,481 rows), `True.csv` (21,417 rows)  
**Total combined:** 44,898 records  

### ğŸ§¹ Cleaning & Normalization Steps
| Step | Description |
|------|--------------|
| Merging & Labeling | Added label column (0 = Fake, 1 = Real). |
| Duplicate Removal | Dropped 6,252 duplicates. |
| Short Text Filter | Removed 144 rows under 50 chars. |
| Text Normalization | Removed URLs, special symbols, and extra whitespace. |
| Column Merge | Combined *title* + *text* into `content` column. |

ğŸ“¦ **Final Shape:** 38,502 rows Ã— 6 columns  
ğŸ—‚ **Cleaned Dataset:** `data/processed/cleaned_combined.csv`

**Label Distribution**
| Class | Label | Proportion |
|--------|--------|-------------|
| Real News | 1 | 55% |
| Fake News | 0 | 45% |

---

## âš™ï¸ Model Architecture & Workflow

### 1ï¸âƒ£ Baseline Model
- **TF-IDF + Logistic Regression**
- Metrics: Accuracy = 0.89, Precision = 0.88, Recall = 0.87  
- Served as interpretability baseline.

### 2ï¸âƒ£ Fine-Tuned Model
- **DistilBERT Base Uncased** (Hugging Face)
- Optimizer: AdamW, Learning Rate: 2e-5, Batch Size: 16  
- Training Epochs: 3  
- Final Accuracy: **0.942** on validation set  
- Metrics visualization: `figures/metrics_comparison_bar.png`, `figures/confusion_matrices_comparison.png`

### 3ï¸âƒ£ Explainability
- **SHAP** used to identify influential tokens.
- Highlighted linguistic cues and emotional patterns driving misclassifications.

---

## ğŸ§© Project Structure

```
B198project/
â”‚
â”œâ”€â”€ app.py                           # Flask app for inference
â”œâ”€â”€ Requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ cleaned_combined.csv
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ Fake.csv
â”‚   â””â”€â”€ True.csv
â”‚
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ metrics_comparison_bar.png
â”‚   â”œâ”€â”€ confusion_matrices_comparison.png
â”‚   â””â”€â”€ total_misclassifications.png
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_preparation.ipynb
â”‚   â”œâ”€â”€ 02_baseline_model.ipynb
â”‚   â”œâ”€â”€ 03_distilbert_finetuning.ipynb
â”‚   â””â”€â”€ 04_evaluation_and_results.ipynb
â”‚
â””â”€â”€ trained_distilbert_fake_news/
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model.safetensors
    â””â”€â”€ training_args.bin
```

---

## âš ï¸ Note on Git LFS Files
This repository uses **Git Large File Storage (LFS)** for large model and dataset files.

Tracked via LFS:
- `data/processed/cleaned_combined.csv`
- `datasets/Fake.csv`
- `datasets/True.csv`
- `trained_distilbert_fake_news/model.safetensors`

If cloning the repo, run:

```bash
git lfs install
git lfs pull
```

---

## ğŸ§° Tools & Libraries
| Category | Libraries |
|-----------|------------|
| Core | Pandas, NumPy, scikit-learn |
| NLP | Transformers, Datasets, Tokenizers |
| ML | PyTorch, Accelerate, Safetensors |
| Visualization | Matplotlib |
| Explainability | SHAP |
| App Layer | Flask |
| Dev Tools | JupyterLab, Git, Git LFS |

---

## ğŸš€ Quick Setup & Run

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/erenbg1/B198_project.git
cd B198_project
```

### 2ï¸âƒ£ Install dependencies
```bash
pip install -r Requirements.txt
```

### 3ï¸âƒ£ Pull LFS files (if needed)
```bash
git lfs install
git lfs pull
```

### 4ï¸âƒ£ Run app
```bash
python app.py
```

or open individual notebooks inside `/notebooks/`.

---

## ğŸ§  Results Summary

| Model | Accuracy | Precision | Recall | F1-Score |
|--------|-----------|------------|---------|-----------|
| TF-IDF + Logistic Regression | 0.89 | 0.88 | 0.87 | 0.88 |
| DistilBERT Fine-Tuned | **0.94** | **0.93** | **0.94** | **0.94** |

Misclassification analysis revealed higher confusion in **neutral-toned articles**, aligning with prior research on human cognitive bias in misinformation detection.

---

## ğŸ‘¤ Author
**Eren Burak GÃ¶kpÄ±nar**  
GISMA University of Applied Sciences  
**Module:** B198 End-to-End Project

---

## ğŸ License
This project is distributed for educational and research purposes under the MIT License.  
See the full license text in `LICENSE` if provided.
